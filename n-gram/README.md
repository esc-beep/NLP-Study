# 2. N-gram Language Modeling

## 2.1 언어 모델이란 무엇일까?

__언어 모델(Language Model)__ 이란 기본적으로 다음에 올 단어를 예측하는 기계 학습 모델이다.
언어 모델 패러다임에는 N-gram language model과 Large language model(LLM)이 있다.

언어 모델의 목표는 단순히 단순히 다음 단어를 맞추는 것을 넘어, 문장 전체의 자연스러움을 확률로 계산하는 것이다.

## 2.2 문장의 확률 계산: Chain Rule

문장 전체의 확률 $P(W)$는 $P(w_1, w_2 ,w_3,...,w_n)으로 나타낸다.

다음에 올 단어의 확률 $P(W_n)$은 $P(w_n|w_1, w_2,...,w_{n−1})$로 나타낸다.
이 공식은 $w_1$부터 $w_{n-1}$까지의 단어들이 주어졌을 때, 다음 단어가 $w_n$일 조건부 확률을 의미한다.

언어 모델이란 이 두 가지 종류의 확률($P(W)$ 또는 $P(w_n|w_1, w_2,...,w_{n−1})$ 중 하나를 계산하는 모델이라고 할 수 있다.

하지만 세상에는 무수히 많은 문장이 존재하므로 특정 문장이 나타날 확률을 계산하기 위해 그 문장이 얼마나 자주 등장했는지 세는 것은 불가능에 가깝다.

이 문제를 해결하기 위해 __확률의 연쇄 법칙 (Chain Rule)__ 을 사용한다.
연쇄 법칙은 문장 전체의 확률을 한 번에 구하는 대신, 각 단어의 확률을 단계별로 곱해나가는 방식이다.

조건부 확률은 $P(B|A) = P(A,B) / P(A)$로 나타낼 수 있고, 이를 두 사건 A와 B가 함께 일어날 확률 $P(A,B)$를 P(B|A) * P(A)$로 변형 가능하다.

이를 4개의 사건(A, B, C, D)에 대해 $P(A, B, C, D) = P(A) * P(B|A) * P(C|A, B) * P(D|A, B, C)$와 같이 확장할 수 있다. 즉, 첫 번째 사건의 확률부터 시작해서, 이전 사건들이 모두 일어났다는 조건 하에 다음 사건이 일어날 확률을 계속해서 곱해나가는 것이다.

이를 일반화하면 다음과 같은 연쇄 법칙 공식이 완성된다.<br>
$P(x_1,x _2​,...,x _n​) = P(x_1)P(x_2​∣x_1​)P(x_3​∣x_1,x_2)...P(x_n​∣x_1,...,x_{n−1}$)
<br><br>
> The water of Walden Pond is so beautifully blue.  

위의 문장을 chain rule을 통해 분석해보자.

$P(W) = P(The, water, of, Walden, Pond, is, so, beautifully, blue)$ 이다.

chain rule을 적용하면 $P(The water or Walden Pond) = P(The) * P(water|the) * P(of|The water) * P(Walden|The water of) * P(Pond|The water of Walden)$으로 나타낼 수 있다.

## 2.3 단순화: 마르코프 가정

이렇게 chain rule을 적용한다고 하더라도, $P(blue|The water of Walden Pond is so beautifully)$를 계산하기 위해서는 그 문장이 나온 모든 경우를 알아야 하기 때문에 문제가 발생한다. (2.2 참고)

__마르코프 가정(Markov Assumption)__ 은 다음에 올 단어는 바로 직전의 몇 개 단어에만 영향을 받는다는 가정이다. 모든 과거를 기억하는 대신, 최근 N개의 단어만 기억하는 일종의 단기 기억 전략이다.
수식으로 표현하면 $P(w_n|w_{1:n-1}) ≈ P(w_n|w_{n-N+1:n-1})$이다.

이 'N'을 몇 개로 하느냐에 따라 N-gram 모델의 종류가 나뉜다.

## 2.4 N-gram 모델의 종류

N-gram에서 'gram'은 단어를, 'N'은 몇 개의 단어를 묶어서 보는지(기억의 범위)를 의미한다.
- 유니그램 (Unigram, 1-gram)
    - N=1
    - $P(w_1w_2...w_n) ≈ P(w_1)P(w_2)...P(w_i)$
- 바이그램 (Bigram, 2-gram)
    - N=2, 바로 앞 단어 1개만 기억
    - $P(w_i | w_{i-1})$

N-gram 모델의 한계는 다음과 같다.
1. 장거리 의존성 문제: 멀리 떨어진 단어의 관계를 파악하기 어려움
2. 새로운 데이터 문제: 학습 데이터에 등장하지 않은 단어 조합이 나오면 계산 결과가 나쁨
이에 대한 해결책으로 embedding space를 이용해 긴 문맥을 잘 학습하는 **Large Language Model**이 사용된다.

## 2.5 N-gram 모델의 확률 계산

N-gram 모델의 확률은 __최대우도추정 (Maximum Likelihood Estimate, MLE)__ 을 통해 계산된다. <br>
MLE는 우리가 가진 데이터(Corpus)에서 가장 많이 관찰된(빈도가 높은) 패턴이 실제로 일어날 확률이 가장 높을 것이라고 가정하는 방식이다.

![Maximum Likelihood Estimate](mle.png)
- $P(w_n∣w_{n−1})$: 단어 $w_{n−1}$ 다음에 단어 $w_n$이 나올 확률
- $C(w_{n−1}​w_n))$: 전체 데이터에서 두 단어 $w_{n-1}$와 $w_n$가 연달아 나온 횟수(Count)
- $C(w_{n−1})$: 전체 데이터에서 앞 단어($w_{n−1}$)가 총 등장한 횟수
즉, "A 다음에 B가 나올 확률 = (A B가 연달아 나온 횟수) / (A가 총 나온 횟수)" 로 계산하는 것이다.
<br><br>

> &lt;s&gt; I am Sam &lt;/s&gt; <br>
> &lt;s&gt; Sam I am &lt;/s&gt; <br>
> &lt;s&gt; I do not like green eggs and ham &lt;/s&gt;

위와 같은 데이터(corpus)가 있을 때 몇 가지 확률을 계산해보자.

$P(I|s)$, 문장이 시작(&lt;s&gt;)된 후 'I'가 나올 확률을 계산해보자.<br>
문장의 시작(&lt;s&gt;)은 총 3번 있었으므로 $C(s) = 3$, &lt;s&gt; 다음 I가 나온 횟수는 2번이므로 $C(s,I) = 2$이다. <br>
따라서 확률은 2/3 = 0.67이다.

실제 연구에 사용되었던 Berkeley Restaurant Project의 데이터를 통해 N-gram 모델의 확률을 계산해보자.

![Raw bigram counts](raw-bigram-counts.png)<br>
위의 이미지는 9,222개의 문장 데이터에서, 어떤 단어 뒤에 어떤 단어가 몇 번 나왔는지를 표로 정리한 것이다.
가로줄(row)의 단어가 먼저 나오고, 세로줄(column)의 단어가 뒤따르는 경우의 횟수이다.

![Raw bigram probabilities](raw-bigram-probabilities.png)<br>
각 단어가 총 몇 번 등장했는지(unigram count)를 확인한 후, `바이그램 횟수 / 유니그램 횟수`로 계산하면 확률을 구할 수 있다.

완성된 바이그램 확률표를 이용해 새로운 문장의 전체 확률을 계산할 수 있다. <br>
"I want english food"의 확률을 계산하고 싶다면 연쇄 법칙을 이용해 다음과 같이 계산할 수 있다. <br>
$P(s I want english food /s)=P(I∣s)×P(want∣I)×P(english∣want)×P(food∣english)×P(/s∣food)$

N-gram을 통해 다음과 같은 지식을 학습할 수 있다.
- 문법 지식: 'want' 다음에는 'to'가 올 확률이 높다는 것을 학습함
- 의미 지식: 'dinner'나 'lunch' 같은 단어는 비슷한 문맥에서 사용된다는 것을 파악함
- 현실 지식: 데이터 안에서 "Chinese food"가 "English food"보다 더 자주 언급된다면, 전자의 확률을 더 높게 평가해 현실 세계의 인기도를 반영함 <br>
<br>

확률은 0과 1 사이의 작은 수라서 계속 곱하면 컴퓨터가 0으로 처리해버리는 '언더플로우' 문제가 생길 수 있다. 이를 방지하기 위해 확률에 로그를 씌워서 곱셈을 덧셈으로 바꿔 계산의 안정성을 높인다.

## 2.6 Larger N-gram

4-gram, 5-gram처럼 N의 크기를 키울수록 더 자연스러운 문맥을 학습할 수 있지만 미리 계산하고 저장해야 할 N-gram의 경우의 수가 기하급수적으로 늘어나 엄청난 저장 공간과 시간이 필요하다.

이를 해결하기 위해 새로운 모델인 __인피니그램 (Infini-grams, ∞-grams)__ 이 등장했다. <br>
인피니그램은 모든 N-gram을 미리 계산해서 저장하는 대신, 5조 개의 단어로 이루어진 거대한 원본 텍스트 전체를 __접미사 배열(Suffix Array)__ 이라는 특별한 데이터 구조로 저장한다. 사용자가 특정 N-gram의 확률을 물어보면, 그때그때 이 자료 구조를 통해 실시간으로 매우 빠르게 계산해서 알려준다. <br>
미리 계산하지 않기 때문에 N의 크기에 제한이 없다.

### 2.6.1 접미사 배열

접미사 배열 (Suffix Arrays)을 "banana"라는 단어의 예시로 이해해보자. <br>

'접미사'는 특정 위치에서부터 문자열의 끝까지를 의미한다. "banana"의 모든 접미사와 시작 위치(index, 0부터 시작)는 다음과 같다.
- 0: banana
- 1: anana
- 2: nana
- 3: ana
- 4: na
- 5: a
<br>

이제 위에서 만든 접미사들을 알파벳순으로 정렬한다.
- a (5)
- ana (3)
- anana (1)
- banana (0)
- na (4)
- nana (2)
<br>

접미사 배열이란 바로 위에서 정렬된 순서대로, 각 접미사의 **'원래 시작 위치'**를 나열한 배열이다.<br>
"banana"의 접미사 배열은 [5, 3, 1, 0, 4, 2]이 된다.
알파벳순으로 정렬되어 있으면 이진 탐색으로 O(logn)만큼의 시간 안에 찾을 수 있어 검색 속도가 빠르다.

## 2.7 언어 모델 평가

언어 모델의 평가 방법에는 크게 두 가지가 있다.
- 외부 평가 (Extrinsic Evaluation): 언어 모델을 실제 서비스에 직접 적용 후 측정
- 내부 평가 (Intrinsic Evaluation): 언어 모델 자체의 성능을 측정 <br>
내부 평가의 경우 퍼플렉서티(Perplexity) 등의 지표로 나타낼 수 있다.
<br><br>
모델을 평가할 때는 데이터를 세 가지 종류로 나누는 것이 일반적이다.<br>

- 훈련 데이터셋 (Training Set): 모델을 학습시키는 데 사용하는 데이터
- 테스트 데이터셋 (Test Set): 학습이 끝난 모델의 최종 성능을 평가하기 위한 데이터
- 개발 데이터셋 (Dev Set): 모델을 개발하는 과정에서 성능을 중간 점검하는 데 사용하는 데이터

## 2.8 Perplexity

__퍼플렉시티(Perplexity)__ 는 언어 모델이 얼마나 '당황하는지' 또는 '헷갈려 하는지'를 수치로 나타낸 것이다. perplexity 점수가 낮을수록 좋은 모델이다.

직관적으로 좋은 언어 모델은 다음과 같다.
- 실제로 자주 쓰이는 문장에는 높은 확률을 부여한다.
- 다음 단어를 잘 예측한다.
- 테스트 문장 전체를 잘 예측한다.
<br> <br>

문장의 확률을 그대로 사용하면 문장이 길어질수록 0과 1 사이의 확률값을 계속 곱하기 때문에 값이 너무 작아져서 비교하기 어렵다는 문제가 있다. <br>
여러 값을 곱한 결과의 평균을 구할 때는 __기하 평균(Geometric Mean)__ 을 사용하며, N제곱근으로 계산할 수 있다. <br>

![Perplexity](perplexity.png)<br>
perplexity는 테스트 문장 전체 확률의 역수에, 문장의 총 단어 수(N)만큼 거듭제곱근을 취한 값이다. <br>
기하 평균을 사용했기 때문에 perplexity를 통해 문장 길이에 상관없이 단어 하나당 평균적으로 얼마나 헷갈려 하는지를 측정할 수 있다. <br>

확률(P)[0, 1]을 최대화하는 것과 퍼플렉시티(PP)[1,∞]를 최소화하는 것은 결국 동일한 의미를 가진다. (역수이므로) <br>
perplexity를 "모델이 다음 단어를 예측할 때, 평균적으로 몇 개의 선택지를 놓고 고민하는가?"로 해석할 수 있다.<br>
<br>
language L = {red, green, blue}, testset T = {red red red red blue}라고 주어졌다고 가정해보자. <br>
$P_A$는 P(red)=.33, p(green)=.33, p(blue)=.33의 확률을 가질 때 $Perplexity_A(T) = ((1/3)^5)^{-1/5} = 3$이다. <br>
$P_B$는 P(red)=.8, p(green)=.1, p(blue)=.1의 확률을 가질 때 $Perplexity_B(T) = (.8 * .8 * .8 * .8 * .1)^{-1/5} = 1.89$이다. <br>
perplexity가 낮은 모델 B가 모델 A보다 선호된다. <br>
<br>
모델이 실제로 어떻게 작동하는지 확인하기 위해서는 **샘플링**을 하는 것이 가장 확실하다.<br>
그 예시 중 하나가 섀넌의 시각화 방법 (The Shannon Visualization Method)인데, 유니그램 모델의 경우에는 문맥을 고려하지 않기 때문에 단순히 단어들을 무작위로 나열한 것으로 보이지만, 바이그램 모델의 경우에는 바로 앞 단어 하나만 고려하기 때문에 짧은 구는 그럴듯하게 만들어낸다.

![Sampling](sampling.png)<br>
샘플링을 하는 방법은 간단하다. 0에서 1까지의 긴 막대에서, 각 단어는 확률만큼 막대의 길이를 나누어 차지한다.<br>
컴퓨터는 0과 1 사이에서 무작위로 숫자 하나를 뽑는다. 만약 0.04라는 숫자가 뽑혔다면, 그 숫자가 속한 구간의 단어인 'the'가 선택되는 원리이다.<br>
<br>

![Bigram Sampling](bigram-sampling.png)<br>
바이그램 문장 생성 과정도 비슷하다.
1. 먼저 문장의 시작(&lt;s&gt;) 다음에 올 단어를 위의 방식으로 뽑는다. (예: 'I'가 뽑힘)
2. 그다음, 'I' 다음에 올 단어들의 확률분포를 가지고 다시 단어를 뽑는다. (예: 'want'가 뽑힘)
3. 이 과정을 계속 반복하다가 문장의 끝(&lt;/s&gt;) 기호가 뽑히면 문장 생성을 멈춘다.<br>
이렇게 만들어진 단어들을 모두 이으면 "I want to eat Chinese food"와 같은 문장이 완성된다.<br> <br>

이렇게 샘플링 기법을 셰익스피어의 작품 전체를 학습한 N-gram 모델에 적용하면, N의 값이 커질수록 문장이 자연스러워지지만 점점 "셰익스피어가 작성한 문장"과 동일한 문장만 나온다. 왜냐하면 셰익스피어의 모든 작품을 학습해도 4-gram의 단어 조합 중 극히 일부만 학습되기 때문에 문장을 복사하는 것처럼 보이게 된다.<br>
이러한 희소성 문제를 해결하기 위해서 유사한 장르의 훈련 corpus를 사용하거나, 다양한 종류의 방언과 화자를 다루는 방식으로 해소할 수 있다.<br>

따라서 N-gram 모델은 훈련 데이터에만 과적합이 되어 훈련 데이터와 조금이라도 다르면 잘 작동하지 않는다.<br>
가장 큰 문제는 훈련 데이터에 나타나지 않은 조합은 확률을 0으로 계산하는 **Zeros**이다.<br>
예를 들어, `ate lunch`, `ate dinner` training set으로 학습했을 때, `ate breakfast`라는 문장이 들어왔을 때 P("breakfast"|ate) = 0으로 계산된다. 그러면 perplexity 공식에 따라 0으로 나누게 되어 계산이 불가능해지고, 모델의 성능 평가가 불가능해진다.<br>
이처럼 훈련 데이터에 없다는 이유만으로 말이 되는 수많은 문장들의 확률을 0으로 계산해버리는 __희소성 문제(Sparsity)로 인한 Zeros__ 가 발생한다.<br>

## 2.9 Smoothing

![Smoothing](smoothing.png)<br>
0의 확률 방식을 해결하기 위한 아이디어가 __Smoothing__ 이다.<br>
Smoothing은 훈련 데이터에서 자주 나왔던 단어들의 횟수를 조금 훔쳐서 한 번도 나오지 않았던 단어들에게 조금씩 나눠준다.<br>
MLE는 N-gram에서 횟수를 세어 확률을 계산하는 방식이기 때문에 __'훈련 데이터'에 가장 잘 들어맞는 확률 추정치__ 이다. 따라서 훈련 데이터에만 너무 최적화되어 있기 때문에 훈련 데이터에 없던 단어가 테스트 데이터에 나오면 제대로 예측하지 못해 일반화 성능이 떨어진다. <br>
Smoothing은 확률을 재분배하기 때문에 훈련 데이터의 통계에서 약간 벗어나지만 일반화 성능이 더 좋아진다.<br>

### 2.9.1 Add-one Smoothing

![Add-one Smoothing](smoothing-addone.png)<br>
가장 직관적으로 smoothing을 구현하는 방법이다. 라플라스(Laplace) 스무딩이라고도 부른다.<br>
모든 N-gram의 횟수에 무조건 1을 더해주는 방식이다. 확률 P를  구할 때는 분자에는 +1을 하고, 분모에는 앞 단어의 총 횟수에 Vocabulary의 개수를 더해주어 구할 수 있다.<br>

![Add-one Smoothing Example](smoothing-addone-example.png)<br>
왼쪽의 표는 훈련 데이터의 모든 바이그램 횟수에 1을 더한 것이고, 오른쪽의 표는 이를 통해 최종 확률을 계산한 것이다.<br>

![Add-one Smoothing Result](smoothing-addone-result.png)<br>
위 표는 Laplace smoothing을 사용하지 않은 경우와 사용한 경우에 두 단어가 연달아 나올 확률을 나타내는 표이다.<br>
해당 표에서는 0이었던 값(i to)이 0.64가 되었다는 것과 높은 횟수(i want)의 값이 827에서 529로 낮아진 **확률의 재분배**가 발생했음을 확인할 수 있다.<br>
이는 자주 나왔던 단어들의 횟수를 조금 훔쳐서(827→529) 한 번도 나오지 않았던 단어들에게 조금씩 나눠주는(0→0.64) 과정이 일어난 것이다.<br>
<br>
Add-one smoothing이 간단하지만, Vocabulary가 커질 수록 너무 많은 확률을 0인 조합에게 나눠줘야 하기 때문에 정작 자주 나왔던 조합의 확률이 필요 이상으로 많이 낮아지는 문제가 발생한다.<br>
따라서 N-gram에서는 보간법(Interpolation)이나 backoff 기법을 사용하고, Add-one Smoothing은 0의 문제가 심각하지 않은 다른 NLP에서 사용된다.<br>

## 2.10 Backoff & Interpolation

**Backoff**는 말 그대로 **이전 모델을 참고**하는 방법이다.
1. 일단 3-gram 확률을 사용한다.
2. 만약 3-gram 정보가 없으면(확률 0), 2-gram 확률을 사용한다.
3. 만약 2-gram 정보가 없으면, 1-gram 확률을 사용한다.<br> <br>

__보간법(Interpolation)__ 은 backoff처럼 하나만 선택하는 것이 아니라, 3-gram, 2-gram, 1-gram의 **확률을 모두 섞어서 사용**한다.<br>
각 N-gram에 가중치를 두어 합치는 방식으로, 일반적으로 Interpolation이 Backoff보다 성능이 좋다.<br>

### 2.10.1 Interpolation

![Interpolation](interpolation.png)<br>
Interpolation을 계산하는 방식은 두 가지로 나뉜다.
1. Simple interpolation: 단순 보간법, 가중치(람다)는 고정된 상수로, 항상 동일한 비율로 3-gram, 2-gram, 1-gram의 확률을 섞는다.
2. Lambdas conditional on context: 문맥 의존적 보간법, 가중치(람다) 값은 함수로, 현재 문맥(= 앞의 두 단어)에 따라 달라진다.<br> <br>

그렇다면 람다 값은 어떻게 정할까?<br>
최고의 람다 비율을 찾기 위해 개발 데이터셋, 즉 **Held-Out 데이터셋**을 사용한다.<br>
먼저 훈련 데이터로 각 N-gram의 확률을 모두 계산해 두고, 이 확률들을 가지고 람다 값을 바꿔가며 Held-out 데이터의 전체 확률을 가장 높게 만드는 최적의 람다 조합을 찾아내는 것이다.<br>

## 2.10.2 Backoff

> P(pahcakes|delicious souffle)를 계산하고 싶다고 가정해보자.
1. 먼저 3-gram(P(pahcakes|delicious souffle))를 확인한다.
2. 3-gram의 확률이 0이라면 2-gram(P(pahcakes|souffle))의 확률을 사용한다.
3. 2-gram의 확률이 0이라면 1-gram(P(pahcakes))의 확률을 사용한다.<br>
실제로는 단순히 낮은 단계의 확률을 그대로 쓰는 것이 아니라, 확률의 총합이 1을 넘지 않도록 할인(discount) 과정을 거친다.<br> <br>

![Stupid Backoff](backoff.png)<br>
할인 과정을 생략한 backoff 방식을 stupid backoff라고 한다.<br>
1. 우리가 찾으려는 N-gram의 횟수가 0보다 크다면 표준적인 MLE 확률을 사용한다.
2. 만약 N-gram의 횟수가 0이라면 한 단계 낮은 N-gram의 점수를 계산하고, 고정된 가중치를 곱한다.
3. 만약 한 단계 낮은 N-gram의 횟수도 0이라면 1-gram에 도달할 때까지 계속 반복한다.<br>
이 방식은 확률의 총합이 1이 되지 않을 수 있어 수학적으로는 진짜 확률이 아니지만 거대한 데이터를 다룰 때 효과적이다.<br>

## 2.11 Summary: 0의 확률 문제(희소성 문제) 해결
1. 스무딩 (Smoothing)
    - 모든 N-gram을 원래 본 횟수보다 1번(또는 k번) 더 봤다고 "가정"하는, 즉 횟수를 인위적으로 더해주는 방식
    - 너무 단순해서 "뭉툭한 도구(blunt instrument)"라고 불리지만, 가끔 유용하게 쓰일 때가 있음
2. 백오프 (Backoff)
    - 만약 3-gram 정보가 없다면, 한 단계 물러나서 2-gram 확률을 대신 사용하는 방식
    - 가중치 계산이 복잡할 수 있지만, '스투피드 백오프'처럼 단순화된 버전은 웹 규모의 초대용량 데이터에서 잘 작동함
3. 보간법 (Interpolation)
    - 3-gram, 2-gram, 1-gram의 확률을 가중치를 두어 모두 "섞는(mix)" 방식
    - 일반적으로 가장 좋은 방법, N-gram뿐만 아니라 최신 LLM들을 섞을 때도 사용됨
